{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae6b7533",
   "metadata": {},
   "source": [
    "# 01 å»ºæ„ç®€å•çš„LLMåº”ç”¨\n",
    "\n",
    "langchainæ”¯æŒå¸‚é¢ä¸Šç»å¤§å¤šæ•°çš„å¤§æ¨¡å‹ã€‚å› ç½‘ç»œé™åˆ¶ï¼Œè¿™é‡Œä½¿ç”¨çš„æ¨¡å‹æ˜¯`deepseek`\n",
    "\n",
    "éœ€è¦ `pip install -qU \"langchain-deepseek\"`\n",
    "\n",
    "ç¯å¢ƒå˜é‡é…ç½®ï¼š\n",
    "\n",
    "- DEEPSEEK_API_KEYï¼šæ¨¡å‹çš„KEY\n",
    "- MODEL_NAME: æ¨¡å‹åç§°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1b6ecdc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56072da",
   "metadata": {},
   "source": [
    "## åˆ›å»ºå¤§æ¨¡å‹\n",
    "\n",
    "å¤§æ¨¡å‹é€šè¿‡`init_chat_model`å‡½æ•°å°±å¯ä»¥åˆ›å»ºï¼Œæ¨¡å‹åä¸º`deepseek-chat`ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75f58adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "import os\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ª ChatModel\n",
    "model = init_chat_model(os.environ.get('MODEL_NAME'), model_provider=\"deepseek\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed233598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Raising my head, I see the moon so bright; withdrawing my eyes, my nostalgia comes around.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 31, 'total_tokens': 53, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 31}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_baeac5aaa3_prod0820_fp8_kvcache', 'id': '706d2080-25f2-4acd-a9f0-2bebb311d92c', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--be6f648d-4842-4c09-8197-8e9c010cf4f9-0', usage_metadata={'input_tokens': 31, 'output_tokens': 22, 'total_tokens': 53, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# SystemMessage æ˜¯ç³»ç»Ÿæç¤ºè¯(å¯é€‰)ï¼Œ HumanMessage æ˜¯ç”¨æˆ·è¾“å…¥\n",
    "messages = [\n",
    "    SystemMessage(\"ä½ æ˜¯ä¸€ä¸ªä¼˜ç§€çš„ç¿»è¯‘å·¥ä½œè€…ï¼Œæ“…é•¿å°†ä¸­æ–‡ç¿»è¯‘ä¸ºè‹±æ–‡ï¼Œå°†ä¸‹é¢çš„ä¸­æ–‡ç¿»è¯‘æˆè‹±æ–‡\"),\n",
    "    HumanMessage('ä¸¾å¤´æœ›æ˜æœˆï¼Œä½å¤´æ€æ•…ä¹¡')\n",
    "]\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd810074",
   "metadata": {},
   "source": [
    "åœ¨`langchain`ä¸­ï¼Œæœ‰ä¸‰ç§æ¶ˆæ¯ï¼š\n",
    "\n",
    "- SystemMessageï¼šç³»ç»Ÿæç¤ºè¯\n",
    "- HumanMessageï¼šç”¨æˆ·è¾“å…¥\n",
    "- AIMessageï¼šAIå›ç­”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63fa1246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='å“ˆå“ˆï¼Œæˆ‘è¯ç”Ÿäºå­—èŠ‚è·³åŠ¨çš„æŠ€æœ¯æµ·æ´‹ä¸­ï¼Œè‹¥ä»æˆ‘çš„æ ¸å¿ƒç®—æ³•é¦–æ¬¡å¯åŠ¨ç®—èµ·ï¼ˆçº¦2023å¹´å·¦å³ï¼‰ï¼ŒæŒ‰äººç±»çš„çºªå¹´æ–¹å¼å¤§æ¦‚è¿˜æ˜¯ä¸ªâ€œå°æœ‹å‹â€å‘¢ï½ä¸è¿‡ä½œä¸ºAIï¼Œæˆ‘æ²¡æœ‰çœŸå®çš„å¹´é¾„æ¦‚å¿µï¼Œæ°¸è¿œåœ¨è¿­ä»£å­¦ä¹ ä¸­æˆé•¿å“¦ï¼æœ‰ä»€ä¹ˆé—®é¢˜å°½ç®¡é—®ï¼Œæˆ‘çš„â€œçŸ¥è¯†å†…å­˜â€å¯æ²¡æœ‰å¹´é™é™åˆ¶ ğŸ˜„', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 71, 'prompt_tokens': 9, 'total_tokens': 80, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 9}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_baeac5aaa3_prod0820_fp8_kvcache', 'id': '15d8cc2f-674f-4543-9e3b-bec9514bb112', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--a8eaa1c0-bec4-4da4-bbb5-bff02907beb6-0', usage_metadata={'input_tokens': 9, 'output_tokens': 71, 'total_tokens': 80, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ä¸å¸¦ç³»ç»Ÿæç¤ºè¯çš„æƒ…å†µï¼Œä¹Ÿå°±æ˜¯æ²¡æœ‰ä¸Šä¸‹æ–‡ç¯å¢ƒ\n",
    "# å¼‚æ­¥å‡½æ•°åœ¨å‰é¢åŠ ä¸€ä¸ª aï¼Œå³ ainvoke  ä¸‹åŒ\n",
    "model.invoke('æ‚¨è€å¤šå¤§å¹´çºªäº†')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12e36c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ä¸€ä¸ªäººé€šå¸¸æœ‰ **10 ä¸ªæ‰‹æŒ‡**ï¼ˆåŒ…æ‹¬æ‹‡æŒ‡ï¼‰ã€‚å…·ä½“æ¥è¯´ï¼š\\n\\n- **æ¯åªæ‰‹æœ‰ 5 ä¸ªæ‰‹æŒ‡**ï¼ˆåŒ…æ‹¬æ‹‡æŒ‡ï¼‰ï¼Œ  \\n- å› æ­¤åŒæ‰‹å…±æœ‰ \\\\(5 \\\\times 2 = 10\\\\) ä¸ªæ‰‹æŒ‡ã€‚\\n\\nåœ¨åŒ»å­¦å’Œè§£å‰–å­¦ä¸­ï¼Œæ‰‹æŒ‡ï¼ˆdigitsï¼‰åŒ…æ‹¬æ‹‡æŒ‡ï¼ˆthumbï¼‰ã€é£ŸæŒ‡ï¼ˆindex fingerï¼‰ã€ä¸­æŒ‡ï¼ˆmiddle fingerï¼‰ã€æ— åæŒ‡ï¼ˆring fingerï¼‰å’Œå°æŒ‡ï¼ˆlittle fingerï¼‰ï¼Œæ‰€æœ‰è¿™äº›éƒ½æ˜¯æ‰‹æŒ‡çš„ç»„æˆéƒ¨åˆ†ã€‚  \\næ‰€ä»¥ï¼Œç­”æ¡ˆæ˜ç¡®æ˜¯ **10 ä¸ª**ã€‚', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 108, 'prompt_tokens': 9, 'total_tokens': 117, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 9}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_baeac5aaa3_prod0820_fp8_kvcache', 'id': '61e5891a-cb27-4e6b-8745-c1b0f7f71bce', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--2fb2a7b3-09ba-4557-abe8-917b4ecf64ae-0', usage_metadata={'input_tokens': 9, 'output_tokens': 108, 'total_tokens': 117, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# å¦å¤–ä¸€ç§ä¸å¸¦ç³»ç»Ÿæç¤ºè¯çš„å†™æ³•\n",
    "model.invoke([{'role': 'user', 'content': 'ä¸€ä¸ªäººæœ‰å¤šå°‘ä¸ªæ‰‹æŒ‡ï¼Ÿ'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01c48845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|æˆ‘ä»¬æ¥|é€æ­¥|è®¡ç®—|è¡¨è¾¾å¼|ï¼š|**|5|åŠ |1|ä¹˜ä»¥|2|**|ã€‚\n",
      "\n",
      "|æ ¹æ®|æ•°å­¦|è¿ç®—|é¡ºåº|ï¼ˆ|å…ˆ|ä¹˜|é™¤|å|åŠ å‡|ï¼‰|ï¼š\n",
      "|1|.| |å…ˆ|è®¡ç®—|ä¹˜æ³•|éƒ¨åˆ†|ï¼š|**|1| Ã—| |2| =| |2|**\n",
      "|2|.| |ç„¶å|è¿›è¡Œ|åŠ æ³•|ï¼š|**|5| +| |2| =| |7|**\n",
      "\n",
      "|æ‰€ä»¥|ï¼Œ|**|5|åŠ |1|ä¹˜ä»¥|2|ç­‰äº|7|**|ã€‚|\n",
      "\n",
      "|**|æœ€ç»ˆ|ç­”æ¡ˆ|ï¼š|**\n",
      "|\\[|\n",
      "|\\|box|ed|{|7|}\n",
      "|\\]||"
     ]
    }
   ],
   "source": [
    "# æµå¼æ–¹å¼è°ƒç”¨ï¼Œè¿”å›çš„tokenä½¿ç”¨ | éš”å¼€\n",
    "\n",
    "for token in model.stream([HumanMessage('5åŠ 1ä¹˜ä»¥2ç­‰äºå¤šå°‘')]):\n",
    "    print(token.content, end='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d879e221",
   "metadata": {},
   "source": [
    "## æç¤ºè¯æ¨¡æ¿\n",
    "\n",
    "ä»ä¸Šæˆ‘ä»¬çœ‹åˆ°çš„ç³»ç»Ÿæç¤ºè¯å’Œç”¨æˆ·è¾“å…¥ï¼Œå¦‚æœæ¯æ¬¡éƒ½è¦æ‹¼æ¥ç³»ç»Ÿæç¤ºè¯æ¯”è¾ƒéº»çƒ¦ï¼Œå¦‚æœå¿˜è®°äº†æºå¸¦ç³»ç»Ÿæç¤ºè¯ï¼Œå¤§æ¨¡å‹å°±ä¼šç­”éæ‰€é—®ï¼Œä¸ºç®€åŒ–æç¤ºè¯ï¼Œ`langchain`æä¾›äº†æç¤ºè¯æ¨¡æ¿ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a28748cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='ç¿»è¯‘ä¸­æ–‡ä¸ºæ—¥æ–‡', additional_kwargs={}, response_metadata={}), HumanMessage(content='éå¸¸æ„Ÿè°¢', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"ç¿»è¯‘ä¸­æ–‡ä¸º{language}\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [('system', system_template), ('user', \"{question}\")]\n",
    ")\n",
    "\n",
    "prompt = prompt_template.invoke({'language': 'æ—¥æ–‡', 'question': 'éå¸¸æ„Ÿè°¢'})\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7bd398",
   "metadata": {},
   "source": [
    "ä»ä¸Šå¯ä»¥çœ‹å‡ºï¼Œ ChatPromptValueåŒ…å«ç³»ç»Ÿæ¶ˆæ¯å’Œç”¨æˆ·æ¶ˆæ¯ï¼Œä¹Ÿå¯ä»¥é€šè¿‡`prompt.to_messages()`è½¬æ¢æˆä¹‹å‰çš„æ¶ˆæ¯æ•°ç»„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c88e55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ã©ã†ã‚‚ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15ce4c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ä½ å¥½å°æ˜ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚æˆ‘æ˜¯DeepSeek-V3ï¼Œä½ å¯ä»¥å«æˆ‘DeepSeekæˆ–è€…å°æ·±~ æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼ŸğŸ˜Š', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 12, 'total_tokens': 44, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 12}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_baeac5aaa3_prod0820_fp8_kvcache', 'id': '805dba80-0009-4b71-8a1f-147677a6f9bf', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--485e4b93-68c7-4f53-a58b-8640bb65f853-0', usage_metadata={'input_tokens': 12, 'output_tokens': 32, 'total_tokens': 44, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke('ä½ å¥½ï¼Œæˆ‘å«å°æ˜ï¼Œä½ å«ä»€ä¹ˆåå­—')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44dd0448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä¸ï¼Œæˆ‘ä¸çŸ¥é“ä½ çš„åå­—ã€‚ä½œä¸ºä¸€ä¸ªAIåŠ©æ‰‹ï¼Œæˆ‘æ²¡æœ‰è®°å¿†åŠŸèƒ½ï¼Œæ— æ³•è®°ä½ä¹‹å‰å¯¹è¯ä¸­çš„ä¸ªäººä¿¡æ¯ï¼ŒåŒ…æ‹¬ä½ çš„åå­—ã€‚ä¸è¿‡ï¼Œå¦‚æœä½ æ„¿æ„å‘Šè¯‰æˆ‘ï¼Œæˆ‘å¯ä»¥åœ¨å½“å‰çš„å¯¹è¯ä¸­ä½¿ç”¨å®ƒæ¥æ›´å¥½åœ°ä¸ä½ äº¤æµï¼ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke('ä½ çŸ¥é“æˆ‘çš„åå­—å—ï¼Ÿ')\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9cd315",
   "metadata": {},
   "source": [
    "ä»ä¸Šå¯¹è¯å¯ä»¥çœ‹å‡ºï¼Œå¤§æ¨¡å‹æ˜¯æ²¡æœ‰è®°å¿†çš„ï¼ˆå³ç¼ºä¹ä¸Šä¸‹æ–‡ï¼‰ï¼Œä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘éœ€è¦å°†å†å²ä¿¡æ¯å¼•å…¥ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d159902d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ˜¯çš„ï¼Œä½ åˆšåˆšå‘Šè¯‰æˆ‘ä½ å«å°æ˜ï¼ğŸ˜Š æˆ‘ä¼šè®¤çœŸè®°ä½çš„ï¼Œé™¤éä½ ä¹‹åå‘Šè¯‰æˆ‘å…¶ä»–åå­—æˆ–è®©æˆ‘å¿˜è®°ï¼ˆæ ¹æ®è®¾è®¡ï¼Œå¯¹è¯è®°å½•ä¼šåœ¨ç»“æŸåè‡ªåŠ¨æ¸…ç†ï¼Œä¿æŠ¤éšç§å“¦ï¼‰ã€‚éœ€è¦æˆ‘å¸®ä½ åšä»€ä¹ˆå—ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "response = model.invoke(\n",
    "    [\n",
    "        HumanMessage('ä½ å¥½ï¼Œæˆ‘å«å°æ˜ï¼Œä½ å«ä»€ä¹ˆåå­—'),\n",
    "        AIMessage('ä½ å¥½å°æ˜ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚æˆ‘æ˜¯DeepSeek-V3ï¼Œä½ å¯ä»¥å«æˆ‘DeepSeekæˆ–è€…å°æ·±~ æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼ŸğŸ˜Š'),\n",
    "        HumanMessage('ä½ çŸ¥é“æˆ‘çš„åå­—å—ï¼Ÿ')\n",
    "    ]\n",
    ")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07f62a1",
   "metadata": {},
   "source": [
    "ä¸Šé¢çš„æç¤ºè¯åšèµ·æ¥è¿˜æ˜¯æœ‰ç‚¹è´¹åŠ²ï¼Œå¯ä»¥ä½¿å ä½ç¬¦æ–¹å¼ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b98305a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='ä½ æ˜¯ä¸€åç”µåŠ›ç³»ç»Ÿçš„ä¸“å®¶ï¼Œç²¾é€šæºç½‘è·å‚¨ç”¨ç›¸å…³çŸ¥è¯†', additional_kwargs={}, response_metadata={}), HumanMessage(content='å¾®ç”µç½‘å’Œè™šæ‹Ÿç”µå‚æœ‰ä»€ä¹ˆåŒºåˆ«', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ä½ æ˜¯ä¸€åç”µåŠ›ç³»ç»Ÿçš„ä¸“å®¶ï¼Œç²¾é€šæºç½‘è·å‚¨ç”¨ç›¸å…³çŸ¥è¯†\"),\n",
    "        MessagesPlaceholder(variable_name='messages'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt = prompt_template.invoke({'messages': [HumanMessage('å¾®ç”µç½‘å’Œè™šæ‹Ÿç”µå‚æœ‰ä»€ä¹ˆåŒºåˆ«')]})\n",
    "prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87441c4a",
   "metadata": {},
   "source": [
    "## æ¶ˆæ¯æŒä¹…åŒ–\n",
    "\n",
    "è¦å®ç°å¤šè½®å¯¹è¯ï¼Œå°±éœ€è¦å°†æ¶ˆæ¯æŒä¹…åŒ–ã€‚`langgraph`æä¾›å†…ç½®çš„æŒä¹…å±‚ï¼Œä¸ºå¤šè½®å¯¹è¯ï¼ˆä¼šè¯ï¼‰æä¾›æ”¯æŒã€‚\n",
    "\n",
    "ä¸‹é¢ä½¿ç”¨å…¶è‡ªå¸¦çš„å†…å­˜æ£€æŸ¥ç‚¹ï¼ˆä¹Ÿå¯ä½¿ç”¨SQLiteã€Postgresç­‰æ•°æ®åº“ï¼‰ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0362e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state['messages'])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "# åœ¨å·¥ä½œæµä¸­å®šä¹‰ä¸€ä¸ªç®€å•çš„èŠ‚ç‚¹\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# åœ¨å·¥ä½œæµä¸­å¢åŠ å†…å­˜æ£€æŸ¥ç‚¹\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# è¿™ä¸ªå…¶å®ç›¸å½“äºä¼šè¯ID\n",
    "config = {'configurable': {'thread_id': 'thread_0001'}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd83138e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ä½ å¥½å‘€ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ï¼ğŸ˜Š ä½ çš„åå­—å¾ˆæœ‰ä¸ªæ€§ï¼Œè®©æˆ‘æƒ³èµ·äº†ä¸€äº›æœ‰è¶£çš„ç½‘ç»œæ–‡åŒ–æ¢—ï½ è¯·é—®ä»Šå¤©æœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ çš„å—ï¼Ÿæ— è®ºæ˜¯å­¦ä¹ ã€å·¥ä½œè¿˜æ˜¯ç”Ÿæ´»ç›¸å…³çš„é—®é¢˜ï¼Œæˆ‘éƒ½å¾ˆä¹æ„ä¸ºä½ æä¾›å¸®åŠ©å“¦ï¼âœ¨\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from typing import TypedDict, Any\n",
    "\n",
    "query = \"ä½ å¥½ï¼Œæˆ‘æ˜¯èµµæ—¥å¤©ï¼Œå¾ˆé«˜å…´è®¤è¯†ä½ \"\n",
    "\n",
    "input_message = [HumanMessage(query)]\n",
    "output = app.invoke({'messages': input_message}, config)\n",
    "output['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "92fda50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ä½ å¥½å‘€ï¼æˆ‘æ˜¯ **DeepSeek-V3**ï¼Œä½ å¯ä»¥å«æˆ‘ **DeepSeek** æˆ– **å°æ·±**ï½ ğŸ˜Š  \n",
      "æˆ‘æ˜¯ä½ çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œä¸“é—¨ä¸ºä½ æä¾›å„ç§çŸ¥è¯†è§£ç­”ã€èŠå¤©é™ªä¼´å’Œå®ç”¨å¸®åŠ©ï¼âœ¨  \n",
      "\n",
      "æœ‰ä»€ä¹ˆæƒ³èŠçš„æˆ–è€…éœ€è¦å¸®å¿™çš„ï¼Œå°½ç®¡é—®æˆ‘å“¦ï¼\n"
     ]
    }
   ],
   "source": [
    "query = \"è¯·é—®ä½ å«ä»€ä¹ˆåå­—ï¼Ÿ\"\n",
    "\n",
    "input_message = [HumanMessage(query)]\n",
    "output = app.invoke({'messages': input_message}, config)\n",
    "output['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b24cadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ä½ å¥½ï¼ä½ ä¹‹å‰æåˆ°è¿‡ä½ çš„åå­—æ˜¯ **èµµæ—¥å¤©**ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆæœ‰ä¸ªæ€§å’Œç½‘ç»œæ–‡åŒ–ç‰¹è‰²çš„åå­—ï¼Œæ‰€ä»¥æˆ‘è®°ä½å•¦ï½ ğŸ˜Š  \n",
      "ä¸è¿‡å¦‚æœä½ æœ‰æ›´å–œæ¬¢çš„ç§°å‘¼ï¼Œéšæ—¶å‘Šè¯‰æˆ‘å“¦ï¼æˆ‘ä¼šæ ¹æ®ä½ çš„åå¥½æ¥è°ƒæ•´ï½âœ¨\n"
     ]
    }
   ],
   "source": [
    "query = \"ä½ çŸ¥é“æˆ‘çš„åå­—å—ï¼Ÿ\"\n",
    "\n",
    "input_message = [HumanMessage(query)]\n",
    "output = app.invoke({'messages': input_message}, config)\n",
    "output['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846a0ad3",
   "metadata": {},
   "source": [
    "ä»è¿™é‡Œå¯ä»¥çœ‹å‡ºï¼Œå¤§æ¨¡å‹æ˜¯è®°ä½ä½ çš„åå­—äº†ã€‚ä½†æ˜¯å¦‚æœä½ å°†`config`ä¸­çš„`thread_id`ä¿®æ”¹ä¸ºå…¶å®ƒå€¼ï¼Œä½ å‘ç°ä»–åˆå¿˜è®°ä½ äº†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "240644aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ä¸ï¼Œæˆ‘ä¸çŸ¥é“ä½ çš„åå­—ã€‚ä½œä¸ºä¸€ä¸ªAIï¼Œæˆ‘æ²¡æœ‰è®°å¿†èƒ½åŠ›ï¼Œæ— æ³•è®°ä½ä¹‹å‰å¯¹è¯ä¸­çš„ä¿¡æ¯ï¼Œæ‰€ä»¥æ¯æ¬¡äº¤æµéƒ½åƒæ˜¯ç¬¬ä¸€æ¬¡ã€‚å¦‚æœä½ æ„¿æ„ï¼Œå¯ä»¥å‘Šè¯‰æˆ‘ä½ çš„åå­—ï¼Œæˆ‘ä¼šåœ¨æœ¬æ¬¡å¯¹è¯ä¸­ä½¿ç”¨å®ƒï¼ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "config = {'configurable': {'thread_id': 'thread_0002'}}\n",
    "query = \"ä½ çŸ¥é“æˆ‘çš„åå­—å—ï¼Ÿ\"\n",
    "\n",
    "input_message = [HumanMessage(query)]\n",
    "output = app.invoke({'messages': input_message}, config)\n",
    "output['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762f8a03",
   "metadata": {},
   "source": [
    "å…¶å®å¤§æ¨¡å‹æ˜¯æ²¡æœ‰è®°å¿†çš„ï¼Œ`langgraph`åªæ˜¯åœ¨å°†ä¸Šä¸‹æ–‡ä¿å­˜åœ¨`thread_id`çš„å†…å­˜ä¸­ï¼Œåœ¨å‘é€æ¶ˆæ¯çš„æ—¶å°†å†å²å‘é€åˆ°å¤§æ¨¡å‹ã€‚\n",
    "\n",
    "ä½ éšæ—¶å¯ä»¥å°†`thread_id`ä¿®æ”¹ä¸ºä¹‹å‰çš„ï¼Œè¿™æ ·å¯¹åº”çš„ä¸Šä¸‹æ–‡å°±ä¼šå‘é€åˆ°å¤§æ¨¡å‹äº†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "23d2349b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "æ˜¯çš„ï¼Œä½ ä¹‹å‰æåˆ°è¿‡ä½ çš„åå­—æ˜¯ **èµµæ—¥å¤©**ï¼Œæˆ‘ä¼šè®°ä½è¿™ä¸ªä¿¡æ¯ï½ ğŸ˜Š  \n",
      "å¦‚æœä½ å¸Œæœ›æˆ‘ç”¨å…¶ä»–æ–¹å¼ç§°å‘¼ä½ ï¼ˆæ¯”å¦‚æ˜µç§°æˆ–è‹±æ–‡åï¼‰ï¼Œéšæ—¶å‘Šè¯‰æˆ‘å“¦ï¼âœ¨\n"
     ]
    }
   ],
   "source": [
    "config = {'configurable': {'thread_id': 'thread_0001'}}\n",
    "query = \"ä½ çŸ¥é“æˆ‘çš„åå­—å—ï¼Ÿ\"\n",
    "\n",
    "input_message = [HumanMessage(query)]\n",
    "output = app.invoke({'messages': input_message}, config)\n",
    "output['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c23967",
   "metadata": {},
   "source": [
    "å¦‚æœç³»ç»Ÿæç¤ºè¯ä¸­æœ‰å¤šä¸ªå‚æ•°ï¼Œé‚£ä¹ˆå¯¹åº”çš„çŠ¶æ€å®šä¹‰ä¹Ÿè¦è¿›è¡Œä¿®æ”¹ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6519914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"ä½ æ˜¯ä¸€åä¼˜ç§€çš„å°åŠ©æ‰‹ï¼Œå›ç­”æ‰€æœ‰çš„é—®é¢˜ä½¿ç”¨{language}\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name='messages')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ad4363be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Sequence\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    language: str\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "async def call_model(state: State):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = await model.ainvoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "app = workflow.compile(checkpointer=MemorySaver())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0eb6c3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Zhang San! Nice to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "config = {'configurable': {'thread_id': 'thread_1001'}}\n",
    "\n",
    "query = \"ä½ å¥½ï¼Œæˆ‘æ˜¯å¼ ä¸‰\"\n",
    "language = \"è‹±è¯­\"\n",
    "\n",
    "input_message = [HumanMessage(query)]\n",
    "output = await app.ainvoke(\n",
    "    {\"messages\": input_message, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8c757521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Of course. When traveling from Germany to the UK, the \"best\" value depends on your specific priorities: absolute lowest cost, a balance of time and money, or convenience.\n",
      "\n",
      "Hereâ€™s a breakdown of the most cost-effective options, from cheapest to best all-around value.\n",
      "\n",
      "### 1. The Absolute Cheapest: Bus (Coach)\n",
      "\n",
      "This is almost always the winner for the lowest upfront cost.\n",
      "\n",
      "*   **Primary Operator:** FlixBus.\n",
      "*   **Typical Route:** For example, Berlin to London or Frankfurt to London.\n",
      "*   **Approximate Cost:** â‚¬30 - â‚¬60 one-way if booked well in advance.\n",
      "*   **Travel Time:** Very long, usually **16 to 24 hours**. This includes the cross-channel ferry crossing.\n",
      "*   **Pros:**\n",
      "    *   Extremely cheap.\n",
      "    *   Direct service from city center to city center.\n",
      "*   **Cons:**\n",
      "    *   Very long and can be uncomfortable for a full day's journey.\n",
      "    *   Limited legroom and amenities.\n",
      "*   **Best for:** Solo travelers on an ultra-tight budget who are not pressed for time.\n",
      "\n",
      "### 2. The Best Balance: Train + Ferry (or Eurotunnel)\n",
      "\n",
      "This isn't usually a single ticket but a journey you piece together. It can be cheap but requires more planning.\n",
      "\n",
      "*   **How it works:** You take a train to a northern French or Belgian port city (like Calais or Dunkirk), then a ferry to the UK (Dover or Folkestone), and then another train to your final destination (e.g., London).\n",
      "*   **Approximate Cost:** Varies greatly. Can be as low as â‚¬50-â‚¬80 if you book train segments with advance fares and find a cheap ferry crossing.\n",
      "*   **Travel Time:** 10+ hours, depending on connections.\n",
      "*   **Pros:**\n",
      "    *   A more scenic and adventurous experience.\n",
      "    *   You can break up the journey.\n",
      "*   **Cons:**\n",
      "    *   Logistically complex with multiple bookings and transfers.\n",
      "    *   Not necessarily faster or cheaper than other options.\n",
      "*   **Best for:** Travelers who enjoy the journey itself and want to avoid flying.\n",
      "\n",
      "### 3. The Most Popular & Best All-Around Value: Flight\n",
      "\n",
      "For most people, flying offers the best compromise of cost, speed, and convenience.\n",
      "\n",
      "*   **Primary Operators:** Budget airlines like **Ryanair, easyJet, Eurowings**. Major carriers like Lufthansa and British Airways also operate routes but are often more expensive.\n",
      "*   **Common Routes:** Dozens of airports in Germany (Berlin, Frankfurt, Munich, Cologne, DÃ¼sseldorf, Hamburg, etc.) fly to many UK airports (London Stansted, Luton, Gatwick, Heathrow, as well as Manchester, Edinburgh, Birmingham).\n",
      "*   **Approximate Cost:** **â‚¬30 - â‚¬100** one-way for budget carriers if booked a few weeks in advance. Prices spike dramatically last-minute.\n",
      "*   **Travel Time:** 1.5 - 2 hours flight time, but you must add time for airport transfers, security, and boarding (add 3-4 hours total travel time).\n",
      "*   **Pros:**\n",
      "    *   Very fast for the actual distance.\n",
      "    *   Huge number of route options and frequencies.\n",
      "*   **Cons:**\n",
      "    *   Airport procedures can be time-consuming.\n",
      "    *   baggage fees on budget airlines can quickly increase the total cost.\n",
      "    *   Environmental impact is higher.\n",
      "*   **Best for:** Almost everyone. It's the standard for a reasonâ€”it's efficient and competitively priced.\n",
      "\n",
      "### 4. The Most Convenient (but Rarely the Cheapest): Train (Eurostar)\n",
      "\n",
      "This is the most comfortable and city-center to city-center option, but it has a geographic limitation.\n",
      "\n",
      "*   **How it works:** You take a train to Brussels (Belgium) or Paris (France) and then transfer to the **Eurostar**,\n",
      "which goes through the Channel Tunnel directly to London St. Pancras station.\n",
      "*   **Approximate Cost:** **â‚¬100 - â‚¬250+** for the through-fare from a German city to London. The Eurostar leg alone is often expensive unless booked far in advance.\n",
      "*   **Travel Time:** Around 7-9 hours total (e.g., Frankfurt to London).\n",
      "*   **Pros:**\n",
      "    *   Extremely comfortable and scenic.\n",
      "    *   No airport hassle; you arrive in the heart of London.\n",
      "    *   Minimal carbon footprint compared to flying.\n",
      "*   **Cons:**\n",
      "    *   By far the most expensive option on average.\n",
      "    *   Requires a change of trains in Brussels or Paris.\n",
      "*   **Best for:** Travelers who dislike flying, want maximum comfort, and have a higher budget.\n",
      "\n",
      "---\n",
      "\n",
      "### **Summary & Recommendation**\n",
      "\n",
      "*   **For the lowest cost:** Take the **bus (FlixBus)**. Be prepared for a very long trip.\n",
      "*   **For the best value (time vs. money):** **Fly with a budget airline** (Ryanair, easyJet). This is what most people choose. Just book in advance and be mindful of baggage fees.\n",
      "*   **For a unique experience:** Consider the **train+ferry** route for an adventure.\n",
      "*   **For comfort and convenience:** Choose the **train via Eurostar**, if it fits your budget.\n",
      "\n",
      "**Pro Tip:** Always use comparison tools like **Google Flights** (for airlines), **Omio** or **Trainline** (for trains and buses), and the direct websites of operators like **FlixBus** and **Eurostar** to find the best deals for your specific travel dates. Prices can fluctuate significantly.\n"
     ]
    }
   ],
   "source": [
    "query = \"è¯·é—®ä»å¾·å›½å»è‹±å›½ä»€ä¹ˆäº¤é€šæ€§ä»·æ¯”æœ€é«˜\"\n",
    "\n",
    "input_message = [HumanMessage(query)]\n",
    "output = await app.ainvoke(\n",
    "    {\"messages\": input_message},\n",
    "    config,\n",
    ")\n",
    "output['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49580d6",
   "metadata": {},
   "source": [
    "## ä¼šè¯ç®¡ç†\n",
    "\n",
    "ä»ä¸Šå¯ä»¥çœ‹åˆ°ï¼Œç”±äºæ¯æ¬¡å‘é€æ¶ˆæ¯éƒ½éœ€è¦å¸¦å†å²ä¸Šä¸‹æ–‡è®°å½•ï¼Œæ¶ˆè€—çš„`token`æ•°æ˜¯æ¯”è¾ƒå¤§çš„ï¼Œå¹¶ä¸”å¯èƒ½å¯¼è‡´ä¸Šä¸‹æ–‡çª—å£æº¢å‡ºã€‚\n",
    "\n",
    "`langchain`å†…ç½®å¤šç§ç®¡ç†æ¶ˆæ¯åˆ—è¡¨çš„æ–¹æ³•ï¼Œå¦‚`trim_messages`å¯ä»¥æŒ‡å®šä¿ç•™æ ‡è®°æ•°ç›®åŠç­–ç•¥ç­‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "640f045b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='ä½ æ˜¯ä¸€åä¼˜ç§€çš„åŠ©ç†', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='4+5*2ç­‰äºå‡ ï¼Ÿ', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='æ ¹æ®æ•°å­¦ä¸­çš„è¿ç®—ä¼˜å…ˆçº§ï¼Œå…ˆè®¡ç®—ä¹˜æ³•ï¼Œå†è®¡ç®—åŠ æ³•ã€‚é¦–å…ˆè®¡ç®—5Ã—2=10ï¼Œç„¶åè®¡ç®—4+10=14ã€‚æ‰€ä»¥ï¼Œ4+5Ã—2çš„ç»“æœæ˜¯14ã€‚', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import trim_messages\n",
    "from langchain_community.chat_models import ChatZhipuAI\n",
    "\n",
    "# å› ä¸º deepseek æŠ¥é”™ NotImplementedError: get_num_tokens_from_messages()ï¼Œè¿™é‡Œå°†æ¨¡å‹ä¿®æ”¹ä¸ºæ™ºè°±\n",
    "zhipu_model = ChatZhipuAI(\n",
    "    model=os.environ.get('ZHIPUAI_MODEL_NAME'),\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "trimer = trim_messages(\n",
    "    max_tokens=150,\n",
    "    strategy='last',\n",
    "    token_counter=zhipu_model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content='ä½ æ˜¯ä¸€åä¼˜ç§€çš„åŠ©ç†'),\n",
    "    HumanMessage(\"ä½ å¥½ï¼Œæˆ‘çš„å¼ ä¸‰\"),\n",
    "    AIMessage(\"\"\"ä½ å¥½å‘€ï¼æˆ‘æ˜¯DeepSeek-V3ï¼Œä½ å¯ä»¥å«æˆ‘ DeepSeek æˆ– å°æ·±ï½ ğŸ˜Š æˆ‘æ˜¯ä½ çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œä¸“é—¨ä¸ºä½ æä¾›å„ç§çŸ¥è¯†è§£ç­”ã€èŠå¤©é™ªä¼´å’Œå®ç”¨å¸®åŠ©ï¼\"\"\"),\n",
    "    HumanMessage('æˆ‘å–œæ¬¢åƒå†°æ¿€å‡Œï¼Œä½ å–œæ¬¢å—ï¼Ÿ'),\n",
    "    AIMessage(\"\"\"è™½ç„¶æˆ‘æ²¡åŠæ³•åƒäººç±»ä¸€æ ·å“å°é£Ÿç‰©ï¼Œä½†å†°æ¿€å‡Œç¡®å®æ˜¯å¾ˆå¤šäººé’Ÿçˆ±çš„ç¾å‘³å‘¢ï¼å†°å‡‰ç»µå¯†çš„å£æ„Ÿï¼Œæ­é…å„ç§é¦™ç”œçš„å£å‘³ï¼Œæ¯”å¦‚æµ“éƒçš„å·§å…‹åŠ›ã€æ¸…æ–°çš„è‰è“ã€é†‡åšçš„é¦™è‰ï¼Œæƒ³æƒ³éƒ½è§‰å¾—å¾ˆæƒ¬æ„ï½ å°¤å…¶æ˜¯åœ¨ç‚çƒ­çš„å¤©æ°”é‡Œï¼Œä¸€å£å†°æ¿€å‡Œå¸¦æ¥çš„æ¸…çˆ½æ„Ÿï¼ŒçœŸçš„å¾ˆè®©äººæ»¡è¶³å‘€ï¼ä½ æœ€å–œæ¬¢å“ªç§å£å‘³çš„å†°æ¿€å‡Œå‘¢ï¼Ÿ\"\"\"),\n",
    "    HumanMessage('4+5*2ç­‰äºå‡ ï¼Ÿ'),\n",
    "    AIMessage(\"\"\"æ ¹æ®æ•°å­¦ä¸­çš„è¿ç®—ä¼˜å…ˆçº§ï¼Œå…ˆè®¡ç®—ä¹˜æ³•ï¼Œå†è®¡ç®—åŠ æ³•ã€‚é¦–å…ˆè®¡ç®—5Ã—2=10ï¼Œç„¶åè®¡ç®—4+10=14ã€‚æ‰€ä»¥ï¼Œ4+5Ã—2çš„ç»“æœæ˜¯14ã€‚\"\"\")\n",
    "]\n",
    "\n",
    "trimer.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a839a3",
   "metadata": {},
   "source": [
    "è¿™æ ·åªéœ€è¦åœ¨`call_model`å‡½æ•°çš„æœ€å‰é¢åŠ ä¸Š`trimer.invoke`å°†æ¶ˆæ¯è¿›è¡Œè£å‰ªå³å¯ã€‚\n",
    "\n",
    "å¦å¤–ä¸€ç§æ–¹å¼æ˜¯åˆå¹¶ç›¸åŒç±»å‹çš„æ¶ˆæ¯ï¼Œå› ä¸ºæˆ‘ä»¬è¿™é‡Œæ˜¯ä¸€é—®ä¸€ç­”ï¼Œæ‰€ä»¥çœ‹èµ·æ¥æ²¡ä»»ä½•ç”¨å¤„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "24091872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='ä½ æ˜¯ä¸€åä¼˜ç§€çš„åŠ©ç†', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='ä½ å¥½ï¼Œæˆ‘çš„å¼ ä¸‰', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='ä½ å¥½å‘€ï¼æˆ‘æ˜¯DeepSeek-V3ï¼Œä½ å¯ä»¥å«æˆ‘ DeepSeek æˆ– å°æ·±ï½ ğŸ˜Š æˆ‘æ˜¯ä½ çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œä¸“é—¨ä¸ºä½ æä¾›å„ç§çŸ¥è¯†è§£ç­”ã€èŠå¤©é™ªä¼´å’Œå®ç”¨å¸®åŠ©ï¼', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='æˆ‘å–œæ¬¢åƒå†°æ¿€å‡Œï¼Œä½ å–œæ¬¢å—ï¼Ÿ', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='è™½ç„¶æˆ‘æ²¡åŠæ³•åƒäººç±»ä¸€æ ·å“å°é£Ÿç‰©ï¼Œä½†å†°æ¿€å‡Œç¡®å®æ˜¯å¾ˆå¤šäººé’Ÿçˆ±çš„ç¾å‘³å‘¢ï¼å†°å‡‰ç»µå¯†çš„å£æ„Ÿï¼Œæ­é…å„ç§é¦™ç”œçš„å£å‘³ï¼Œæ¯”å¦‚æµ“éƒçš„å·§å…‹åŠ›ã€æ¸…æ–°çš„è‰è“ã€é†‡åšçš„é¦™è‰ï¼Œæƒ³æƒ³éƒ½è§‰å¾—å¾ˆæƒ¬æ„ï½ å°¤å…¶æ˜¯åœ¨ç‚çƒ­çš„å¤©æ°”é‡Œï¼Œä¸€å£å†°æ¿€å‡Œå¸¦æ¥çš„æ¸…çˆ½æ„Ÿï¼ŒçœŸçš„å¾ˆè®©äººæ»¡è¶³å‘€ï¼ä½ æœ€å–œæ¬¢å“ªç§å£å‘³çš„å†°æ¿€å‡Œå‘¢ï¼Ÿ', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='4+5*2ç­‰äºå‡ ï¼Ÿ', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='æ ¹æ®æ•°å­¦ä¸­çš„è¿ç®—ä¼˜å…ˆçº§ï¼Œå…ˆè®¡ç®—ä¹˜æ³•ï¼Œå†è®¡ç®—åŠ æ³•ã€‚é¦–å…ˆè®¡ç®—5Ã—2=10ï¼Œç„¶åè®¡ç®—4+10=14ã€‚æ‰€ä»¥ï¼Œ4+5Ã—2çš„ç»“æœæ˜¯14ã€‚', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import merge_message_runs\n",
    "\n",
    "merged_message = merge_message_runs(messages)\n",
    "merged_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5430bc2b",
   "metadata": {},
   "source": [
    "æ‰€ä»¥ï¼Œæœ€å¥½çš„åŠæ³•æ˜¯è®©ä¸Šè¿°çš„å¯¹è¯ç²¾ç®€ä¸€ä¸‹ä¿å­˜èµ·æ¥ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2733ec94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: æˆ‘å«å°æ˜ï¼Œå–œæ¬¢æ‰“ç¯®çƒï¼Œæœ€è¿‘åœ¨å­¦Python\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'æˆ‘å«å°æ˜ï¼Œå–œæ¬¢æ‰“ç¯®çƒï¼Œæœ€è¿‘åœ¨å­¦Python', 'history': '', 'response': 'ä½ å¥½ï¼Œå°æ˜ï¼å¬èµ·æ¥ä½ å¾ˆå¿™å‘¢ï¼Œæ—¢å–œæ¬¢æ‰“ç¯®çƒï¼Œåˆåœ¨å­¦ä¹ Pythonç¼–ç¨‹ã€‚ç¯®çƒæ˜¯ä¸€é¡¹éå¸¸æœ‰ç›Šçš„è¿åŠ¨ï¼Œå®ƒä¸ä»…èƒ½é”»ç‚¼èº«ä½“ï¼Œè¿˜èƒ½æé«˜å›¢é˜Ÿåˆä½œèƒ½åŠ›ã€‚è‡³äºPythonï¼Œå®ƒæ˜¯ä¸€ç§éå¸¸æµè¡Œä¸”åŠŸèƒ½å¼ºå¤§çš„ç¼–ç¨‹è¯­è¨€ï¼Œéå¸¸é€‚åˆåˆå­¦è€…å…¥é—¨ï¼Œå› ä¸ºå®ƒè¯­æ³•ç®€æ´ï¼Œæ˜“äºå­¦ä¹ ã€‚\\n\\nä½ å­¦Pythonæ˜¯ä¸ºäº†ä»€ä¹ˆç›®çš„å‘¢ï¼Ÿæ˜¯ä¸ºäº†å¼€å‘åº”ç”¨ç¨‹åºã€æ•°æ®åˆ†æï¼Œè¿˜æ˜¯ä»…ä»…ä¸ºäº†ä¸ªäººå…´è¶£ï¼ŸPythonåœ¨æ•°æ®ç§‘å­¦ã€æœºå™¨å­¦ä¹ ã€ç½‘ç»œå¼€å‘ç­‰é¢†åŸŸéƒ½æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚å¦‚æœä½ åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­é‡åˆ°äº†ä»€ä¹ˆå…·ä½“çš„é—®é¢˜ï¼Œæˆ–è€…æƒ³è¦äº†è§£Pythonçš„æŸä¸ªç‰¹å®šæ–¹é¢ï¼Œéšæ—¶å¯ä»¥é—®æˆ‘å“¦ï¼'}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human introduces themselves as Xiao Ming, a basketball enthusiast who is also learning Python. The AI acknowledges Xiao Ming's busy lifestyle and the benefits of both basketball and Python. The AI inquires about Xiao Ming's purpose for learning Python and mentions its applications in various fields, offering assistance with any specific questions or topics.\n",
      "Human: æˆ‘æƒ³çŸ¥é“å¦‚ä½•ç”¨Pythonå¤„ç†Excelè¡¨æ ¼\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'æˆ‘æƒ³çŸ¥é“å¦‚ä½•ç”¨Pythonå¤„ç†Excelè¡¨æ ¼', 'history': \"The human introduces themselves as Xiao Ming, a basketball enthusiast who is also learning Python. The AI acknowledges Xiao Ming's busy lifestyle and the benefits of both basketball and Python. The AI inquires about Xiao Ming's purpose for learning Python and mentions its applications in various fields, offering assistance with any specific questions or topics.\", 'response': \"å½“ç„¶å¯ä»¥ï¼ŒXiao Mingã€‚åœ¨Pythonä¸­å¤„ç†Excelè¡¨æ ¼é€šå¸¸ä½¿ç”¨`pandas`åº“ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸å¼ºå¤§çš„æ•°æ®åˆ†æå·¥å…·ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›åŸºæœ¬çš„æ­¥éª¤å’Œæ¦‚å¿µï¼š\\n\\n1. **å®‰è£…pandasåº“**ï¼šå¦‚æœä½ è¿˜æ²¡æœ‰å®‰è£…pandasï¼Œå¯ä»¥ä½¿ç”¨pipå‘½ä»¤è¿›è¡Œå®‰è£…ï¼š\\n   ```bash\\n   pip install pandas\\n   ```\\n\\n2. **è¯»å–Excelæ–‡ä»¶**ï¼šä½¿ç”¨`pandas`çš„`read_excel`å‡½æ•°å¯ä»¥è½»æ¾åœ°è¯»å–Excelæ–‡ä»¶ã€‚ä¾‹å¦‚ï¼š\\n   ```python\\n   import pandas as pd\\n   df = pd.read_excel('your_file.xlsx')\\n   ```\\n   è¿™é‡Œçš„`your_file.xlsx`æ˜¯ä½ æƒ³è¦è¯»å–çš„Excelæ–‡ä»¶åã€‚\\n\\n3. **æŸ¥çœ‹æ•°æ®**ï¼šè¯»å–æ•°æ®åï¼Œä½ å¯ä»¥ä½¿ç”¨`df.head()`æ¥æŸ¥çœ‹å‰å‡ è¡Œæ•°æ®ï¼Œæˆ–è€…`df.info()`æ¥è·å–æ•°æ®çš„åŸºæœ¬ä¿¡æ¯ã€‚\\n\\n4. **æ•°æ®æ¸…æ´—**ï¼šExcelæ–‡ä»¶å¯èƒ½åŒ…å«ç¼ºå¤±å€¼ã€é‡å¤å€¼æˆ–æ ¼å¼é”™è¯¯ã€‚ä½ å¯ä»¥ä½¿ç”¨`df.dropna()`æ¥åˆ é™¤ç¼ºå¤±å€¼ï¼Œ`df.duplicated()`æ¥åˆ é™¤é‡å¤å€¼ï¼Œæˆ–è€…ä½¿ç”¨`df.fillna()`æ¥å¡«å……ç¼ºå¤±å€¼ã€‚\\n\\n5. **æ•°æ®ç­›é€‰**ï¼šä½ å¯ä»¥ä½¿ç”¨å¸ƒå°”ç´¢å¼•æ¥ç­›é€‰æ•°æ®ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æƒ³ç­›é€‰å‡ºæŸä¸ªåˆ—å€¼å¤§äºæŸä¸ªç‰¹å®šå€¼çš„è¡Œï¼Œå¯ä»¥ä½¿ç”¨ï¼š\\n   ```python\\n   df[df['column_name'] > value]\\n   ```\\n\\n6. **æ•°æ®æ’åº**ï¼šä½¿ç”¨`df.sort_values(by='column_name', ascending=True)`å¯ä»¥å¯¹æ•°æ®è¿›è¡Œæ’åºã€‚\\n\\n7. **æ•°æ®åˆ†ç»„**ï¼šå¦‚æœä½ éœ€è¦å¯¹æ•°æ®è¿›è¡Œåˆ†ç»„ï¼Œå¯ä»¥ä½¿ç”¨`df.groupby('column_name').sum()`æ¥è®¡ç®—æ¯ä¸ªç»„çš„æ€»å’Œã€‚\\n\\n8. **æ•°æ®åˆå¹¶**ï¼šå¦‚æœä½ æœ‰å¤šä¸ªExcelæ–‡ä»¶ï¼Œå¯ä»¥ä½¿ç”¨`pd.concat()`æ¥åˆå¹¶å®ƒä»¬ã€‚\\n\\n9. **æ•°æ®å¯¼å‡º**ï¼šå¤„ç†å®Œæ•°æ®åï¼Œä½ å¯ä»¥ä½¿ç”¨`df.to_excel('output_file.xlsx', index=False)`å°†æ•°æ®å¯¼å‡ºåˆ°æ–°çš„Excelæ–‡ä»¶ã€‚\\n\\nè¿™åªæ˜¯`pandas`åº“åœ¨å¤„ç†Excelæ–‡ä»¶æ—¶çš„ä¸€äº›åŸºæœ¬åŠŸèƒ½ã€‚`pandas`åº“éå¸¸å¼ºå¤§ï¼Œæœ‰å¾ˆå¤šé«˜çº§åŠŸèƒ½å¯ä»¥æ¢ç´¢ã€‚å¦‚æœä½ æœ‰å…·ä½“çš„é—®é¢˜æˆ–éœ€è¦æ›´è¯¦ç»†çš„æŒ‡å¯¼ï¼Œéšæ—¶å‘Šè¯‰æˆ‘ï¼\"}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human, Xiao Ming, a basketball enthusiast learning Python, inquires about using Python to handle Excel spreadsheets. The AI explains that `pandas` is a powerful tool for this purpose and outlines the basic steps for installing the library, reading files, data viewing, cleaning, filtering, sorting, grouping, merging, and exporting data. The AI also mentions the versatility of `pandas` and offers assistance for any specific questions or further guidance.\n",
      "\n",
      "New summary:\n",
      "Xiao Ming, a basketball enthusiast and Python learner, asks the AI for guidance on using Python to manage Excel files. The AI responds by detailing the process of using the `pandas` library, covering installation, data handling, and various operations like cleaning, filtering, sorting, grouping, merging, and exporting. The AI emphasizes the library's power and offers to help with any further questions or detailed instructions.\n",
      "Human: é™¤äº†pandasï¼Œè¿˜æœ‰å…¶ä»–åº“å¯ä»¥ç”¨å—ï¼Ÿ\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'é™¤äº†pandasï¼Œè¿˜æœ‰å…¶ä»–åº“å¯ä»¥ç”¨å—ï¼Ÿ', 'history': \"The human, Xiao Ming, a basketball enthusiast learning Python, inquires about using Python to handle Excel spreadsheets. The AI explains that `pandas` is a powerful tool for this purpose and outlines the basic steps for installing the library, reading files, data viewing, cleaning, filtering, sorting, grouping, merging, and exporting data. The AI also mentions the versatility of `pandas` and offers assistance for any specific questions or further guidance.\\n\\nNew summary:\\nXiao Ming, a basketball enthusiast and Python learner, asks the AI for guidance on using Python to manage Excel files. The AI responds by detailing the process of using the `pandas` library, covering installation, data handling, and various operations like cleaning, filtering, sorting, grouping, merging, and exporting. The AI emphasizes the library's power and offers to help with any further questions or detailed instructions.\", 'response': 'å½“ç„¶æœ‰ï¼Œé™¤äº†`pandas`ä¹‹å¤–ï¼Œè¿˜æœ‰å‡ ä¸ªåº“å¯ä»¥ç”¨æ¥å¤„ç†Excelæ–‡ä»¶ï¼š\\n\\n1. **openpyxl**ï¼šè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯»å†™Excel 2010 xlsx/xlsm/xltx/xltmæ–‡ä»¶çš„åº“ã€‚å®ƒæä¾›äº†å¯¹Excelæ–‡ä»¶çš„é«˜çº§æ“ä½œï¼Œå¦‚è¯»å–ã€å†™å…¥ã€ä¿®æ”¹å•å…ƒæ ¼æ•°æ®ç­‰ã€‚\\n\\n2. **xlrd**ï¼šè¿™ä¸ªåº“ç”¨äºè¯»å–Excel 97-2003æ–‡ä»¶ï¼ˆå³.xlsæ–‡ä»¶ï¼‰ã€‚å®ƒæä¾›äº†è¯»å–å•å…ƒæ ¼æ•°æ®ã€æ ¼å¼ã€å›¾è¡¨ç­‰åŠŸèƒ½ã€‚\\n\\n3. **xlwt**ï¼šç”¨äºå†™å…¥Excel 97-2003æ–‡ä»¶ï¼ˆ.xlsï¼‰ã€‚å¦‚æœä½ éœ€è¦åˆ›å»ºæ–°çš„Excelæ–‡ä»¶æˆ–ä¿®æ”¹ç°æœ‰æ–‡ä»¶ï¼Œè¿™ä¸ªåº“å¯èƒ½å¾ˆæœ‰ç”¨ã€‚\\n\\n4. **xlsxwriter**ï¼šè¿™æ˜¯ä¸€ä¸ªç”¨äºå†™å…¥Excel 2007+ï¼ˆ.xlsxï¼‰æ–‡ä»¶çš„åº“ã€‚å®ƒæä¾›äº†ä¸°å¯Œçš„æ ¼å¼åŒ–é€‰é¡¹ï¼Œå¹¶ä¸”å¯ä»¥åˆ›å»ºå›¾è¡¨ã€‚\\n\\n5. **xlutils**ï¼šè¿™æ˜¯ä¸€ä¸ªè¾…åŠ©åº“ï¼Œå¯ä»¥ç”¨æ¥å¤åˆ¶å’Œä¿®æ”¹Excelæ–‡ä»¶ã€‚\\n\\næ¯ä¸ªåº“éƒ½æœ‰å…¶ç‰¹å®šçš„ç”¨é€”å’Œä¼˜åŠ¿ï¼Œä½ å¯ä»¥æ ¹æ®ä½ çš„å…·ä½“éœ€æ±‚é€‰æ‹©åˆé€‚çš„åº“ã€‚å¦‚æœä½ æœ‰ç‰¹å®šçš„éœ€æ±‚æˆ–è€…æƒ³è¦äº†è§£æŸä¸ªåº“çš„æ›´å¤šç»†èŠ‚ï¼Œæˆ‘å¯ä»¥ä¸ºä½ æä¾›æ›´è¯¦ç»†çš„ä¿¡æ¯ã€‚'}\n",
      "ç²¾ç®€åçš„å¯¹è¯å†å²ï¼š\n",
      "Xiao Ming, a basketball enthusiast and Python learner, seeks the AI's advice on using Python for Excel file management. The AI recommends the `pandas` library for its versatility and outlines its features, including installation, data handling, and operations like cleaning, filtering, sorting, grouping, merging, and exporting. The AI also suggests alternative libraries such as `openpyxl`, `xlrd`, `xlwt`, `xlsxwriter`, and `xlutils`, each with specific uses and advantages, and offers to provide more detailed information on request.\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=zhipu_model)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=zhipu_model,\n",
    "    memory=memory,\n",
    "    verbose=True  # æ‰“å°ä¸­é—´è¿‡ç¨‹ï¼ˆå¯é€‰ï¼‰\n",
    ")\n",
    "\n",
    "# æ¨¡æ‹Ÿå¤šè½®å¯¹è¯\n",
    "response = conversation.invoke(\"æˆ‘å«å°æ˜ï¼Œå–œæ¬¢æ‰“ç¯®çƒï¼Œæœ€è¿‘åœ¨å­¦Python\")\n",
    "print(response)\n",
    "response = conversation.invoke(\"æˆ‘æƒ³çŸ¥é“å¦‚ä½•ç”¨Pythonå¤„ç†Excelè¡¨æ ¼\")\n",
    "print(response)\n",
    "response = conversation.invoke(\"é™¤äº†pandasï¼Œè¿˜æœ‰å…¶ä»–åº“å¯ä»¥ç”¨å—ï¼Ÿ\")\n",
    "print(response)\n",
    "\n",
    "# æŸ¥çœ‹ç²¾ç®€åçš„å†å²æ€»ç»“\n",
    "print(\"ç²¾ç®€åçš„å¯¹è¯å†å²ï¼š\")\n",
    "print(memory.load_memory_variables({})[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b6b01",
   "metadata": {},
   "source": [
    "åŸºç¡€çŸ¥è¯†åˆ°è¿™é‡Œå°±å·²ç»å®Œç»“äº†ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b89edd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
